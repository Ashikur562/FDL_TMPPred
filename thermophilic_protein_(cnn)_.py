# -*- coding: utf-8 -*-
"""Thermophilic Protein (CNN) .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16B9ZzQvzOQLelaVZw6_JdnPNf8V_XF7B
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import os

from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import matthews_corrcoef
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.feature_selection import mutual_info_classif

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import f_classif

from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import ExtraTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier


from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.linear_model import LogisticRegression

from sklearn.svm import SVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis

from keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout,LSTM, Input
from keras.layers import Conv1D, LSTM, BatchNormalization, MaxPooling1D, Dense, Reshape
from keras.models import Sequential
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import log_loss

from transformers import ViTModel, ViTConfig

file_name = r'/kaggle/working/model-1-result.csv'
validation_file = r'/kaggle/working/model-1-validation.csv'

"""# Model Evaluation"""

# for test set
def total_matric_cal(ytest,ypred,pred,model_name):
    pred_clean = np.nan_to_num(pred, nan=0.0)  # Replace NaNs with 0 in pred
    ytest_cnn_clean = np.nan_to_num(ytest_cnn, nan=0.0)  # Replace NaNs with 0 in ytest_cnn
    Metrics = []
    Metrics = pd.DataFrame(Metrics)
    Metrics['Model_name'] = 'model_name'
    Metrics['Accuracy'] = 'Accuracy'
    Metrics['mcc'] = 'mcc'
    Metrics['Kappa'] = 'Kappa'
    Metrics['precision'] = 'precision'
    Metrics['recall'] = 'recall'
    Metrics['f1'] = 'f1'
    Metrics['sensitivity'] = 'sensitivity'
    Metrics['specificity'] = 'specificity'
    Metrics['auc'] = 'auc'
    Metrics['loss'] = 'loss'
    Accuracy = accuracy_score(ytest,ypred)
    mcc = matthews_corrcoef(ytest,ypred)
    cm1 = confusion_matrix(ytest,ypred)
    kappa = cohen_kappa_score(ytest,ypred)
    f1 = f1_score(ytest,ypred, average='macro')
    precision = precision_score(ytest,ypred, average='macro')
    recall = recall_score(ytest,ypred, average='macro')
    specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])
    sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])
    auc = roc_auc_score(ytest_cnn_clean, pred_clean, multi_class='ovr')
    # Binary Cross-Entropy Loss
    loss = log_loss(ytest_cnn_clean, pred_clean)
    Metrics.loc[len(Metrics.index)] = [model_name,Accuracy, mcc, kappa, precision,recall, f1, sensitivity,specificity,auc,loss]
    return Metrics

# for valadtion set
def func(model,model_name,xval,yval):
  pred_val  = model.predict(xval)
  y_pred_classes_val  = np.round(pred_val).astype(int)
  pred_clean = np.nan_to_num(pred_val, nan=0.0)  # Replace NaNs with 0 in pred
  ytest_cnn_clean = np.nan_to_num(yval, nan=0.0)  # Replace NaNs with 0 in ytest_cnn
  Metrics = []
  Metrics = pd.DataFrame(Metrics)
  Metrics['Model_name'] = 'model_name'
  Metrics['Accuracy'] = 'Accuracy'
  Metrics['mcc'] = 'mcc'
  Metrics['Kappa'] = 'Kappa'
  Metrics['precision'] = 'precision'
  Metrics['recall'] = 'recall'
  Metrics['f1'] = 'f1'
  Metrics['sensitivity'] = 'sensitivity'
  Metrics['specificity'] = 'specificity'
  Metrics['auc'] = 'auc'
  Metrics['loss'] = 'loss'
  Accuracy = accuracy_score(y_val,y_pred_classes_val)
  mcc = matthews_corrcoef(y_val,y_pred_classes_val)
  cm1 = confusion_matrix(y_val,y_pred_classes_val)
  kappa = cohen_kappa_score(y_val,y_pred_classes_val)
  f1 = f1_score(y_val,y_pred_classes_val, average='macro')
  precision = precision_score(y_val,y_pred_classes_val, average='macro')
  recall = recall_score(y_val,y_pred_classes_val, average='macro')
  specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])
  sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])
  auc = roc_auc_score(y_val,y_pred_classes_val, multi_class='ovr')
  loss = log_loss(y_val,y_pred_classes_val)
  Metrics.loc[len(Metrics.index)] = [model_name,Accuracy, mcc, kappa, precision,recall, f1, sensitivity,specificity,auc,loss]
  return Metrics

def save_metrics(metrics_df, file_path):
    """
    Check if a file exists and contains data. If not, write the data; otherwise, append it.

    Args:
        metrics_df (DataFrame): The DataFrame containing metrics to save.
        file_path (str): Path to the CSV file.
    """
    # Check if the file exists and has data
    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
        # Append to the file
        metrics_df.to_csv(file_path, mode='a', index=False, header=False)
        print(f"Appended data to {file_path}.")
    else:
        # Write to the file
        metrics_df.to_csv(file_path, index=False)
        print(f"Created and wrote data to {file_path}.")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, MaxPooling1D, Flatten, Dense
from tensorflow.keras.optimizers import Adam

def build_model(input_shape, filters=64, kernel_size=2, pool_size=2, learning_rate=1e-3):
    """
    Function to define a CNN model.

    Args:
        input_shape (tuple): Shape of the input data (e.g., (num_features, 1)).
        filters (int): Number of filters for Conv1D layers.
        kernel_size (int): Kernel size for Conv1D layers.
        pool_size (int): Pool size for MaxPooling1D layers.
        learning_rate (float): Learning rate for the optimizer.

    Returns:
        model (Sequential): A compiled Keras Sequential model.
    """
    model = Sequential([
        # Input Layer
        Input(shape=input_shape),

        # First Conv1D Block
        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'),
        BatchNormalization(),
        MaxPooling1D(pool_size=pool_size),

        # Second Conv1D Block
        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'),
        BatchNormalization(),
        MaxPooling1D(pool_size=pool_size),

        # Fully Connected Layers
        Flatten(),
        Dense(units=filters, activation='relu'),
        Dense(units=1, activation='sigmoid')  # Output layer for binary classification
    ])

    # Compile the model
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model

"""# **APAAC+DDE**"""

df = pd.read_csv('/content/APAAC+DDE.csv')
columns = df.columns.tolist()
# Filter the columns to remove data we do not want
columns = [c for c in columns if c not in ["Target"]]
# Store the variable we are predicting
target = "Target"
X = df[columns]
Y = df[target]

xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.20,random_state=42)

xtrain_cnn = xtrain.to_numpy()
ytrain_cnn = ytrain.to_numpy()
xtrain_cnn = xtrain_cnn.reshape(xtrain.shape[0], xtrain.shape[1], 1)
xtest_cnn = xtest.to_numpy()
ytest_cnn = ytest.to_numpy()
xtest_cnn = xtest_cnn.reshape(xtest_cnn.shape[0], xtest_cnn.shape[1], 1)

model_no = "model1"
model_name = "APAAC"
filters = 64
kernel = 2
pool_size = 2
lstm_units = filters*2

epochs = 100
batch_size = 32
learning_rate = 1e-3

# Initialize KFold Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# To store metrics for all validation folds
all_validation_metrics = pd.DataFrame()

# Perform KFold Cross-Validation
fold_no = 1
for train_index, val_index in kf.split(xtrain_cnn):
    # Split data into training and validation sets
    X_train, X_val = xtrain_cnn[train_index], xtrain_cnn[val_index]
    y_train, y_val = ytrain_cnn[train_index], ytrain_cnn[val_index]

    # Build a new model for this fold
    model = build_model(input_shape=(X_train.shape[1], 1))

    # Early stopping to avoid overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)
    # Train the model on the training fold
    print(f"Training fold {fold_no}...")
    model.fit(X_train, y_train,
              validation_data=(X_val, y_val),
              epochs=epochs,
              batch_size=batch_size,
              callbacks=[early_stopping, reduce_lr],
              verbose=1)

    # Evaluate on validation fold using func()
    validation_metrics = func(model, f"Model_Fold_{fold_no}", X_val, y_val)
    all_validation_metrics = pd.concat([all_validation_metrics, validation_metrics], ignore_index=True)

    fold_no += 1

# Cross-validation results
print("\nValidation Metrics for All Folds:")
print(all_validation_metrics)

# Average validation metrics across folds
mean_validation_metrics = all_validation_metrics.mean(numeric_only=True)
print("\nAverage Validation Metrics Across All Folds:")
print(mean_validation_metrics)

# Train the final model on the entire training set
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)
print("\nTraining the final model on the full training set...")
final_model = build_model(input_shape=(xtrain_cnn.shape[1], 1))



# Train the final model
history = final_model.fit(xtrain_cnn, ytrain_cnn, epochs=epochs, batch_size=batch_size, validation_split=0.1,
              callbacks=[early_stopping, reduce_lr], verbose=1)

# Evaluate the final model on the test set using total_matric_cal()
print("\nEvaluating the final model on the test set...")
pred = final_model.predict(xtest_cnn)  # Predict probabilities
y_pred_classes = np.round(pred).astype(int)  # Convert to binary class labels

test_metrics = total_matric_cal(ytest_cnn, y_pred_classes, pred, "Final_Model_Test_Set")
print("\nTest Set Metrics:")
print(test_metrics)

"""# DDE+PAAC"""

df = pd.read_csv('/content/DDE+PAAC.csv')
columns = df.columns.tolist()
# Filter the columns to remove data we do not want
columns = [c for c in columns if c not in ["Target"]]
# Store the variable we are predicting
target = "Target"
X = df[columns]
Y = df[target]

xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.20,random_state=42)

xtrain_cnn = xtrain.to_numpy()
ytrain_cnn = ytrain.to_numpy()
xtrain_cnn = xtrain_cnn.reshape(xtrain.shape[0], xtrain.shape[1], 1)
xtest_cnn = xtest.to_numpy()
ytest_cnn = ytest.to_numpy()
xtest_cnn = xtest_cnn.reshape(xtest_cnn.shape[0], xtest_cnn.shape[1], 1)

model_no = "model1"
model_name = "CTDC"
filters = 64
kernel = 2
pool_size = 2
lstm_units = filters*2

epochs = 100
batch_size = 32
learning_rate = 1e-3

# Initialize KFold Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# To store metrics for all validation folds
all_validation_metrics = pd.DataFrame()

# Perform KFold Cross-Validation
fold_no = 1
for train_index, val_index in kf.split(xtrain_cnn):
    # Split data into training and validation sets
    X_train, X_val = xtrain_cnn[train_index], xtrain_cnn[val_index]
    y_train, y_val = ytrain_cnn[train_index], ytrain_cnn[val_index]

    # Define the model
    model = build_model(input_shape=(X_train.shape[1], 1))

    # Early stopping to avoid overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)
    # Train the model on the training fold
    print(f"Training fold {fold_no}...")
    model.fit(X_train, y_train,
              validation_data=(X_val, y_val),
              epochs=epochs,
              batch_size=batch_size,
              callbacks=[early_stopping, reduce_lr],
              verbose=1)

    # Evaluate on validation fold using func()
    validation_metrics = func(model, f"Model_Fold_{fold_no}", X_val, y_val)
    all_validation_metrics = pd.concat([all_validation_metrics, validation_metrics], ignore_index=True)

    fold_no += 1

# Cross-validation results
print("\nValidation Metrics for All Folds:")
print(all_validation_metrics)

# Average validation metrics across folds
mean_validation_metrics = all_validation_metrics.mean(numeric_only=True)
print("\nAverage Validation Metrics Across All Folds:")
print(mean_validation_metrics)

# Train the final model on the entire training set
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)
print("\nTraining the final model on the full training set...")
final_model = build_model(input_shape=(xtrain_cnn.shape[1], 1))

# Train the final model
history = final_model.fit(xtrain_cnn, ytrain_cnn, epochs=epochs, batch_size=batch_size, validation_split=0.1,
              callbacks=[early_stopping, reduce_lr], verbose=1)

# Evaluate the final model on the test set using total_matric_cal()
print("\nEvaluating the final model on the test set...")
pred = final_model.predict(xtest_cnn)  # Predict probabilities
y_pred_classes = np.round(pred).astype(int)  # Convert to binary class labels

test_metrics = total_matric_cal(ytest_cnn, y_pred_classes, pred, "Final_Model_Test_Set")
print("\nTest Set Metrics:")
print(test_metrics)

"""# APAAC+PAAC"""

df = pd.read_csv('/content/APAAC+PAAC.csv')
columns = df.columns.tolist()
# Filter the columns to remove data we do not want
columns = [c for c in columns if c not in ["Target"]]
# Store the variable we are predicting
target = "Target"
X = df[columns]
Y = df[target]

xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.20,random_state=42)
xtrain_cnn = xtrain.to_numpy()
ytrain_cnn = ytrain.to_numpy()
xtrain_cnn = xtrain_cnn.reshape(xtrain.shape[0], xtrain.shape[1], 1)
xtest_cnn = xtest.to_numpy()
ytest_cnn = ytest.to_numpy()
xtest_cnn = xtest_cnn.reshape(xtest_cnn.shape[0], xtest_cnn.shape[1], 1)

model_no = "model1"
model_name = "DDE"
filters = 64
kernel = 2
pool_size = 2
lstm_units = filters*2

epochs = 100
batch_size = 32
learning_rate = 1e-3

# Initialize KFold Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# To store metrics for all validation folds
all_validation_metrics = pd.DataFrame()

# Perform KFold Cross-Validation
fold_no = 1
for train_index, val_index in kf.split(xtrain_cnn):
    # Split data into training and validation sets
    X_train, X_val = xtrain_cnn[train_index], xtrain_cnn[val_index]
    y_train, y_val = ytrain_cnn[train_index], ytrain_cnn[val_index]

    # Define the model
    model = build_model(input_shape=(X_train.shape[1], 1))

    # Early stopping to avoid overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)
    # Train the model on the training fold
    print(f"Training fold {fold_no}...")
    model.fit(X_train, y_train,
              validation_data=(X_val, y_val),
              epochs=epochs,
              batch_size=batch_size,
              callbacks=[early_stopping, reduce_lr],
              verbose=1)

    # Evaluate on validation fold using func()
    validation_metrics = func(model, f"Model_Fold_{fold_no}", X_val, y_val)
    all_validation_metrics = pd.concat([all_validation_metrics, validation_metrics], ignore_index=True)

    fold_no += 1

# Cross-validation results
print("\nValidation Metrics for All Folds:")
print(all_validation_metrics)

# Average validation metrics across folds
mean_validation_metrics = all_validation_metrics.mean(numeric_only=True)
print("\nAverage Validation Metrics Across All Folds:")
print(mean_validation_metrics)

# Train the final model on the entire training set
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)
print("\nTraining the final model on the full training set...")
final_model = build_model(input_shape=(xtrain_cnn.shape[1], 1))

# Train the final model
history = final_model.fit(xtrain_cnn, ytrain_cnn, epochs=epochs, batch_size=batch_size, validation_split=0.1,
              callbacks=[early_stopping, reduce_lr], verbose=1)

# Evaluate the final model on the test set using total_matric_cal()
print("\nEvaluating the final model on the test set...")
pred = final_model.predict(xtest_cnn)  # Predict probabilities
y_pred_classes = np.round(pred).astype(int)  # Convert to binary class labels

test_metrics = total_matric_cal(ytest_cnn, y_pred_classes, pred, "Final_Model_Test_Set")
print("\nTest Set Metrics:")
print(test_metrics)

"""# FF"""

df = pd.read_csv(r'/content/FF.csv')
columns = df.columns.tolist()
# Filter the columns to remove data we do not want
columns = [c for c in columns if c not in ["Target"]]
# Store the variable we are predicting
target = "Target"
X = df[columns]
Y = df[target]

xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.20,random_state=42)
xtrain_cnn = xtrain.to_numpy()
ytrain_cnn = ytrain.to_numpy()
xtrain_cnn = xtrain_cnn.reshape(xtrain.shape[0], xtrain.shape[1], 1)
xtest_cnn = xtest.to_numpy()
ytest_cnn = ytest.to_numpy()
xtest_cnn = xtest_cnn.reshape(xtest_cnn.shape[0], xtest_cnn.shape[1], 1)

model_no = "model1"
model_name = "DDE"
filters = 64
kernel = 2
pool_size = 2
lstm_units = filters*2

epochs = 100
batch_size = 32
learning_rate = 1e-3

# Initialize KFold Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# To store metrics for all validation folds
all_validation_metrics = pd.DataFrame()

# Perform KFold Cross-Validation
fold_no = 1
for train_index, val_index in kf.split(xtrain_cnn):
    # Split data into training and validation sets
    X_train, X_val = xtrain_cnn[train_index], xtrain_cnn[val_index]
    y_train, y_val = ytrain_cnn[train_index], ytrain_cnn[val_index]

    model = build_model(input_shape=(X_train.shape[1], 1))

    # Early stopping to avoid overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)
    # Train the model on the training fold
    print(f"Training fold {fold_no}...")
    model.fit(X_train, y_train,
              validation_data=(X_val, y_val),
              epochs=epochs,
              batch_size=batch_size,
              callbacks=[early_stopping, reduce_lr],
              verbose=1)

    # Evaluate on validation fold using func()
    validation_metrics = func(model, f"Model_Fold_{fold_no}", X_val, y_val)
    all_validation_metrics = pd.concat([all_validation_metrics, validation_metrics], ignore_index=True)

    fold_no += 1

# Cross-validation results
print("\nValidation Metrics for All Folds:")
print(all_validation_metrics)

# Average validation metrics across folds
mean_validation_metrics = all_validation_metrics.mean(numeric_only=True)
print("\nAverage Validation Metrics Across All Folds:")
print(mean_validation_metrics)

# Train the final model on the entire training set
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)
print("\nTraining the final model on the full training set...")
final_model = build_model(input_shape=(xtrain_cnn.shape[1], 1))

# Train the final model
history = final_model.fit(xtrain_cnn, ytrain_cnn, epochs=epochs, batch_size=batch_size, validation_split=0.1,
              callbacks=[early_stopping, reduce_lr], verbose=1)

# Evaluate the final model on the test set using total_matric_cal()
print("\nEvaluating the final model on the test set...")
pred = final_model.predict(xtest_cnn)  # Predict probabilities
y_pred_classes = np.round(pred).astype(int)  # Convert to binary class labels

test_metrics = total_matric_cal(ytest_cnn, y_pred_classes, pred, "Final_Model_Test_Set")
print("\nTest Set Metrics:")
print(test_metrics)