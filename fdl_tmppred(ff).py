# -*- coding: utf-8 -*-
"""FDL_TMPPred(FF).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y6NB0pAmQzXrXnNdjpweh9JeJgTHf2iA
"""

!pip install tensorflow

# --------------------
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense
from sklearn.metrics import matthews_corrcoef, cohen_kappa_score
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import GRU, Dropout, Dense, Input, Reshape, MaxPooling1D
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score
from keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout,LSTM
from sklearn.model_selection import KFold

# Enable eager execution
tf.config.run_functions_eagerly(True)
tf.config.experimental_run_functions_eagerly(True)

"""# **Train**"""

# Load your data
train = pd.read_csv('/content/FF_Train -1.csv')

xtrain = train.drop(['Target'], axis=1)
ytrain = train['Target']

# Reshape X_train for Conv1D
X_train1 = xtrain.to_numpy().reshape(xtrain.shape[0], xtrain.shape[1], 1)
Y_train1 = ytrain.to_numpy()

# Define KFold for cross-validation
kf = KFold(n_splits=5, random_state=1, shuffle=True)

for train_index, val_index in kf.split(X_train1):
    X_train_fold, X_val_fold = X_train1[train_index], X_train1[val_index]
    Y_train_fold, Y_val_fold = Y_train1[train_index], Y_train1[val_index]

# --------------------
# Balanced client simulation
NUM_CLIENTS = 10

# Separate positive and negative samples
pos_idx = np.where(Y_train1 == 1)[0]
neg_idx = np.where(Y_train1 == 0)[0]

# Shuffle
np.random.shuffle(pos_idx)
np.random.shuffle(neg_idx)

# Split into equal parts
pos_split = np.array_split(pos_idx, NUM_CLIENTS)
neg_split = np.array_split(neg_idx, NUM_CLIENTS)

clients = []
for i in range(NUM_CLIENTS):
    idx = np.concatenate([pos_split[i], neg_split[i]])
    np.random.shuffle(idx)
    clients.append({
        'x': X_train1[idx],
        'y': Y_train1[idx]
    })

# Print distribution per client
for i, client in enumerate(clients):
    print(f"Client {i+1}: total={len(client['y'])}, positives={np.sum(client['y'])}, negatives={len(client['y']) - np.sum(client['y'])}")

# --------------------
# Create CNN_BiGRU model
def build_model():
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=(X_train1.shape[1], 1)))
    model.add(MaxPooling1D(2))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(MaxPooling1D(2))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Reshape((8, 8)))
    model.add(LSTM(32, return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(64, return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(128, return_sequences=False))
    model.add(Dropout(0.3))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.3)),
    model.add(Dense(1, activation='sigmoid'))
    return model

# --------------------
# Federated training
def federated_training(clients, rounds=25, batch_size=64, lr=0.001):
    global_model = build_model()
    loss_fn = tf.keras.losses.BinaryCrossentropy()

    for r in range(rounds):
        client_weights = []
        for client in clients:
            # Copy global model to client
            local_model = build_model()
            local_model.set_weights(global_model.get_weights())
            local_model.compile(optimizer=Adam(learning_rate=lr), loss=loss_fn, metrics=['accuracy'])

            # Train locally
            local_model.fit(client['x'], client['y'], epochs=1, batch_size=batch_size, verbose=0)

            # Collect updated weights
            client_weights.append(local_model.get_weights())

        # Trimmed Mean (10%)
        trim_ratio = 0.2
        new_weights = []
        for weights in zip(*client_weights):
            sorted_w = np.sort(weights, axis=0)
            k = int(trim_ratio * len(weights))
            trimmed = sorted_w[k:len(weights)-k]  # drop extremes
            new_weights.append(np.mean(trimmed, axis=0))
        global_model.set_weights(new_weights)


        print(f"Round {r+1}/{rounds} complete")

    return global_model

# --------------------
# Train global model
global_model = federated_training(clients, rounds=25)

# --------------------
# Evaluate
global_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
preds = (global_model.predict(X_val_fold) > 0.5).astype(int).squeeze()

cm = confusion_matrix(Y_val_fold, preds)
specificity = cm[0,0]/(cm[0,0]+cm[0,1])

print("\nConfusion Matrix:\n", cm)
print("Accuracy :", accuracy_score(Y_val_fold, preds))
print("Sensitivity (Recall):", recall_score(Y_val_fold, preds, zero_division=0))
print("Specificity:", specificity)
print("MCC      :", matthews_corrcoef(Y_val_fold, preds))
print("F1       :", f1_score(Y_val_fold, preds, zero_division=0))
print("Precision:", precision_score(Y_val_fold, preds, zero_division=0))
print("Kappa    :", cohen_kappa_score(Y_val_fold, preds))

"""# **Test**"""

# Load your data
train = pd.read_csv('/content/FF_Train -1.csv')
test = pd.read_csv('/content/FF_Test -1.csv')

xtrain = train.drop(['Target'], axis=1)
ytrain = train['Target']
xtest = test.drop(['Target'], axis=1)
ytest = test['Target']

sample_size = xtrain.shape[0]
time_steps = xtrain.shape[1]
input_dimension = 1

train_data_reshaped = xtrain.values.reshape(sample_size, time_steps, input_dimension)
n_timesteps = train_data_reshaped.shape[1]
n_features  = train_data_reshaped.shape[2]

xtest_reshaped = xtest.values.reshape(xtest.shape[0], time_steps, input_dimension)

# --------------------
# Balanced client simulation
NUM_CLIENTS = 10

# Separate positive and negative samples
pos_idx = np.where(ytrain.values == 1)[0]
neg_idx = np.where(ytrain.values == 0)[0]

# Shuffle
np.random.shuffle(pos_idx)
np.random.shuffle(neg_idx)

# Split into equal parts
pos_split = np.array_split(pos_idx, NUM_CLIENTS)
neg_split = np.array_split(neg_idx, NUM_CLIENTS)

clients = []
for i in range(NUM_CLIENTS):
    idx = np.concatenate([pos_split[i], neg_split[i]])
    np.random.shuffle(idx)
    clients.append({
        'x': train_data_reshaped[idx],
        'y': ytrain.values[idx]
    })

# Print distribution per client
for i, client in enumerate(clients):
    print(f"Client {i+1}: total={len(client['y'])}, positives={np.sum(client['y'])}, negatives={len(client['y']) - np.sum(client['y'])}")

# --------------------
# Create CNN_BiGRU model
def build_model():
    model = Sequential()
    model.add(Conv1D(32, 3, activation='relu', padding='same', input_shape=(n_timesteps, n_features)))
    model.add(MaxPooling1D(2))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(Conv1D(128, 3, activation='relu', padding='same'))
    model.add(Conv1D(64, 3, activation='relu', padding='same'))
    model.add(MaxPooling1D(2))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Reshape((8, 8)))
    model.add(LSTM(32, return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(64, return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(128, return_sequences=False))
    model.add(Dropout(0.3))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.3)),
    model.add(Dense(1, activation='sigmoid'))
    return model

# --------------------
# Federated training
def federated_training(clients, rounds=25, batch_size=64, lr=0.001):
    global_model = build_model()
    loss_fn = tf.keras.losses.BinaryCrossentropy()

    for r in range(rounds):
        client_weights = []
        for client in clients:
            # Copy global model to client
            local_model = build_model()
            local_model.set_weights(global_model.get_weights())
            local_model.compile(optimizer=Adam(learning_rate=lr), loss=loss_fn, metrics=['accuracy'])

            # Train locally
            local_model.fit(client['x'], client['y'], epochs=1, batch_size=batch_size, verbose=0)

            # Collect updated weights
            client_weights.append(local_model.get_weights())

        # Trimmed Mean (10%)
        trim_ratio = 0.2
        new_weights = []
        for weights in zip(*client_weights):
            sorted_w = np.sort(weights, axis=0)
            k = int(trim_ratio * len(weights))
            trimmed = sorted_w[k:len(weights)-k]  # drop extremes
            new_weights.append(np.mean(trimmed, axis=0))
        global_model.set_weights(new_weights)


        print(f"Round {r+1}/{rounds} complete")

    return global_model

# --------------------
# Train global model
global_model = federated_training(clients, rounds=25)

# --------------------
# Evaluate
global_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
preds = (global_model.predict(xtest_reshaped) > 0.5).astype(int).squeeze()

cm = confusion_matrix(ytest, preds)
specificity = cm[0,0]/(cm[0,0]+cm[0,1])

print("\nConfusion Matrix:\n", cm)
print("Accuracy :", accuracy_score(ytest, preds))
print("Sensitivity (Recall):", recall_score(ytest, preds, zero_division=0))
print("Specificity:", specificity)
print("MCC      :", matthews_corrcoef(ytest, preds))
print("F1       :", f1_score(ytest, preds, zero_division=0))
print("Precision:", precision_score(ytest, preds, zero_division=0))
print("Kappa    :", cohen_kappa_score(ytest, preds))